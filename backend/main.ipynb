{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ecdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # pymupdf\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed text\n",
    "def embed(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return F.normalize(embedding, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9fda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs: 3953\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "# Step 1: Download the book\n",
    "url = \"https://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "raw = request.urlopen(url).read().decode(\"utf8\")\n",
    "\n",
    "# Step 2: Strip the Project Gutenberg boilerplate\n",
    "start = raw.find(\"PART I\")\n",
    "end = raw.rfind(\"End of the Project Gutenberg EBook\")\n",
    "text = raw[start:end]\n",
    "\n",
    "# Step 3: Count paragraphs (split by double newlines)\n",
    "paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "print(f\"Total paragraphs: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0602c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_embeddings = embed(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_data = [\n",
    "    {\n",
    "        \"paragraph\": para,\n",
    "        \"embedding\": embedding.tolist()  # Convert torch.Tensor to Python list\n",
    "    }\n",
    "    for para, embedding in zip(paragraphs, paragraph_embeddings)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8912\n",
      "Paragraph: “And what if we do catch him?”\n",
      "\n",
      "Score: 0.8773\n",
      "Paragraph: “Well, they will catch him.”\n",
      "\n",
      "Score: 0.8521\n",
      "Paragraph: “Nothing easier. It is in just such stupid things clever people are most\n",
      "easily caught. The more cunning a man is, the less he suspects that he\n",
      "will be caught in a simple thing. The more cunning a man is, the simpler\n",
      "the trap he must be caught in. Porfiry is not such a fool as you\n",
      "think....”\n",
      "\n",
      "Score: 0.8340\n",
      "Paragraph: It was only in that that he recognised his criminality, only in the fact\n",
      "that he had been unsuccessful and had confessed it.\n",
      "\n",
      "Score: 0.8316\n",
      "Paragraph: At first--long before indeed--he had been much occupied with one\n",
      "question; why almost all crimes are so badly concealed and so easily\n",
      "detected, and why almost all criminals leave such obvious traces? He\n",
      "had come gradually to many different and curious conclusions, and in his\n",
      "opinion the chief reason lay not so much in the material impossibility\n",
      "of concealing the crime, as in the criminal himself. Almost every\n",
      "criminal is subject to a failure of will and reasoning power by a\n",
      "childish and phenomenal heedlessness, at the very instant when prudence\n",
      "and caution are most essential. It was his conviction that this eclipse\n",
      "of reason and failure of will power attacked a man like a disease,\n",
      "developed gradually and reached its highest point just before the\n",
      "perpetration of the crime, continued with equal violence at the moment\n",
      "of the crime and for longer or shorter time after, according to the\n",
      "individual case, and then passed off like any other disease. The\n",
      "question whether the disease gives rise to the crime, or whether the\n",
      "crime from its own peculiar nature is always accompanied by something of\n",
      "the nature of disease, he did not yet feel able to decide.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"when is he finally caught\"\n",
    "query_embedding = embed([query])[0]  # Shape: (384,)\n",
    "\n",
    "# Find the smallest distance between the query embed and the embeds in the list.\n",
    "\n",
    "\n",
    "# Compute similarity scores between query and all paragraphs\n",
    "scores = torch.matmul(paragraph_embeddings, query_embedding)  # Shape: (N,)\n",
    "\n",
    "\n",
    "top_k = 5\n",
    "top_indices = torch.topk(scores, k=top_k).indices  # Top-k most similar\n",
    "\n",
    "results = [\n",
    "    {\n",
    "        \"score\": scores[i].item(),\n",
    "        \"paragraph\": paragraphs[i],\n",
    "        \"index\": i\n",
    "    }\n",
    "    for i in top_indices\n",
    "]\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Score: {result['score']:.4f}\")\n",
    "    print(f\"Paragraph: {result['paragraph']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d6c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
